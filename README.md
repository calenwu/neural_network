## Task
Given 3 images tell wheter the first image is more likely to taste like the second or third picture.

## Solution
Almost all code comes from this tutorial:
https://keras.io/examples/vision/siamese_network/#image-similarity-estimation-using-a-siamese-network-with-a-triplet-loss

We use a Siamese Network for this task. It contains 3 subnetworks which are used to generate feature vectors for each input and compares them.
B0 uses 224x224 images so: I preprocess the images that is load the jpg, convert them into rgb values and resize them to 224x224

The SiameseNetwork (DistanceLayer) receive 3 images, generate the embeddings and output the distances between the 3 images.
The SiameseModel computes the triples loss in the training loop using the embeddings generated by the SiameseNetwork.


Unlike the tutorial I use EfficientNet (https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2) as my pretrained base
instead of ResNet because according to this image it should perform better:
https://1.bp.blogspot.com/-oNSfIOzO8ko/XO3BtHnUx0I/AAAAAAAAEKk/rJ2tHovGkzsyZnCbwVad-Q3ZBnwQmCFsgCEwYBhgL/s1600/image3.png
I tried to use the keras.applications.efficientnet_v2.EfficientNetV2 one but it does some weird stuff with the output shape, it will add a 7x7 shape to it.
I also made it trainable because through testing it seemed to get better results.
Then I added a Dropout layer at the top to prevent overfitting. Then I added 2 DenseLayers with 4096 units to seperate the embeddings.
There was no particular reason for the choice of units or the Dropout rate or the ordering of those, I just checked via submissions and took the one which got the best result.
I added a l2 normalization layer because for those vectors the squared Euclidean distance is proportional to the cosine distance and in the tutorial they use the cosine distance to evaulate the results.
I got this hint from the IML channel in discord.

Then we use the Adam optimizer with a learning rate of 0.00001. The tutorial uses 0.0001 but through testing I found out 0.00001 got better results.


